# -*- coding: utf-8 -*-
"""pycaret me project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tclMa7VnsxT2fJnxQC6DxaGbPU09P1OJ
"""

! pip install pycaret streamlit pyvirtualdisplay pandas

import streamlit as st
import pycaret.classification as pc
import pycaret.regression as pr
import pandas as pd
import sqlite3
import json
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.pipeline import Pipeline



# Set the background color to light coffee
st.markdown(
    """
    <style>
    body {
        background-color: #f3e0c6;
    }
    </style>
    """,
    unsafe_allow_html=True
)

# Create a Streamlit app
st.title("PyCaret Streamlit App")

# Sidebar for dataset input and configuration
st.sidebar.header("Dataset Configuration")

# Ask the user to provide the dataset type
dataset_type = st.sidebar.selectbox("Select the type of dataset", ["CSV", "Excel", "SQL", "JSON"])

import pycaret.classification as pc
from pycaret.classification import setup
import pandas as pd
import sqlite3
import json

# Ask the user to provide the dataset type
dataset_type = input("Enter the type of dataset (CSV, Excel, SQL, or JSON): ").lower()

# Load the dataset based on the provided type
if dataset_type == 'csv':
    dataset_path = input("Enter the path to your CSV dataset: ")
    data = pd.read_csv(dataset_path)
elif dataset_type == 'excel':
    dataset_path = input("Enter the path to your Excel dataset: ")
    data = pd.read_excel(dataset_path)
elif dataset_type == 'sql':
    db_path = input("Enter the path to your SQLite database: ")
    conn = sqlite3.connect(db_path)
    query = input("Enter your SQL query to fetch the data: ")
    data = pd.read_sql_query(query, conn)
    conn.close()
elif dataset_type == 'json':
    dataset_path = input("Enter the path to your JSON dataset: ")
    with open(dataset_path, 'r') as json_file:
        json_data = json.load(json_file)
        data = pd.DataFrame(json_data)
else:
    print("Unsupported dataset type. Please provide CSV, Excel, SQL, or JSON.")
    exit()

# Display column names and data types
print("Column Names and Data Types:")
print(data.dtypes)

# Ask the user for columns to drop
columns_to_drop = input("Enter columns to drop (comma-separated): ").split(',')
data.drop(columns=columns_to_drop, inplace=True)

# Handle missing values based on user input
for col in data.columns:
    if data[col].dtype == 'object':
        # Handle categorical columns
        fill_method = input(f"What to do with missing values in '{col}' (most_frequent / additional_class / skip): ").strip()
        if fill_method == 'most_frequent':
            data[col].fillna(data[col].mode()[0], inplace=True)
        elif fill_method == 'additional_class':
            data[col].fillna('Missing', inplace=True)
    else:
        # Handle numeric (continuous) columns
        fill_method = input(f"What to do with missing values in '{col}' (mean / median / mode / skip): ").strip()
        if fill_method == 'mean':
            data[col].fillna(data[col].mean(), inplace=True)
        elif fill_method == 'median':
            data[col].fillna(data[col].median(), inplace=True)
        elif fill_method == 'mode':
            data[col].fillna(data[col].mode()[0], inplace=True)

import pycaret.regression as pr

# Initialize PyCaret with the dataset
target_column_name = input("Enter the name of the target column: ")
clf = pc.setup(data, target=target_column_name, session_id=123, normalize=True, transformation=True)

# Handle missing values
data.dropna(inplace=True)  # Remove rows with missing values, you can customize this based on your needs

# classification or regression
if data[target_column_name].dtype in ['int64', 'float64']:
    # Assuming it's regression if the target column contains numerical values
    task_type = 'regression'
else:
    # Assuming it's classification if the target column contains non-numerical values
    task_type = 'classification'

print(f"Detected task type: {task_type}")

if task_type == 'classification':
    clf = pc.setup(data, target=target_column_name, session_id=123, normalize=True, transformation=True)
    # Perform classification tasks using PyCaret
    best_model = pc.compare_models()
    # Rest of your classification-related code...
elif task_type == 'regression':
    reg = pr.setup(data, target=target_column_name, session_id=123, normalize=True, transformation=True)
    # Perform regression tasks using PyCaret
    best_model = pr.compare_models()
    # Rest of your regression-related code...
else:
    print("Unsupported task type. Please check the target column data type.")
    exit()

# Perform any desired data preprocessing and model training using PyCaret's functions
best_model = pc.compare_models()

from pycaret.classification import setup, compare_models, tune_model
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.pipeline import Pipeline

# Perform feature selection (you can choose a different method)
best_features = compare_models(fold=5, sort='F1')

# Create a custom pipeline with feature selection and Logistic Regression
pipeline = Pipeline([
    ('select', SelectKBest(score_func=f_classif)),
    ('lr', LogisticRegression())
])

# Define a custom grid for hyperparameter tuning
param_grid = {
    'select__k': [1, 2, 3, 4, 5],  # Adjust the number of selected features
    'lr__C': [0.001, 0.01, 0.1, 1, 10],  # Adjust regularization strength
}

# Tune the custom pipeline with the custom grid
tuned_lr = tune_model(pipeline, custom_grid=param_grid)

# Create and train multiple classification models
model1 = pc.create_model('lr')
plot_model(model1)

model2 = pc.create_model('rf')
plot_model(model2)

model3 = pc.create_model('xgboost')
plot_model(model3)

# Blend the classification models
blended_model = pc.blend_models(estimator_list=[model1, model2, model3])
plot_model(blended_model)

# Stack the classification models
stacked_model = pc.stack_models(estimator_list=[model1, model2], meta_model=model3)
plot_model(stacked_model)

from pycaret.classification import tune_model

# Tune the model using the 'random' tuner without specifying a search library
tuned_model = tune_model(blended_model, n_iter=10)
plot_model(tuned_model)

from pycaret.classification import evaluate_model

evaluate_model(tuned_model)

"""#### Testing Streamlit"""

import streamlit as st

st.write("Hello, Streamlit!")

!cd /content/sample_data

!streamlit run --browser.gatherUsageStats false pycaret_me_project.py